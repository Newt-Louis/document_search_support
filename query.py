# query.py
import logging
import sys
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.embeddings.fastembed import FastEmbedEmbedding
from llama_index.llms.ollama import Ollama
import qdrant_client
from llama_index.core.prompts import PromptTemplate

# 1. Cáº¥u hÃ¬nh y há»‡t nhÆ° lÃºc Ingest (Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»“ng bá»™ vector)
Settings.embed_model = FastEmbedEmbedding(
    model_name="bge-m3",
    cache_dir="data/cache/multilingual-e5-large"
)

# Cáº¥u hÃ¬nh LLM: Llama-3.2 cháº¡y local
Settings.llm = Ollama(
    model="llama3.2",
    request_timeout=360.0,
    temperature=0.1  # Giá»¯ nhiá»‡t Ä‘á»™ tháº¥p Ä‘á»ƒ model tráº£ lá»i trung thá»±c, Ã­t sÃ¡ng táº¡o linh tinh
)

# 2. Káº¿t ná»‘i láº¡i vÃ o Qdrant (Chá»‰ connect, khÃ´ng táº¡o má»›i)
client = qdrant_client.QdrantClient(url="http://localhost:6333")
vector_store = QdrantVectorStore(client=client, collection_name="company_docs")

# 3. Load Index tá»« Vector DB lÃªn (SiÃªu nháº¹, khÃ´ng tá»‘n RAM load data gá»‘c)
index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

# 4. Cáº¥u hÃ¬nh Prompt "Tháº§n thÃ¡nh" (System Prompt)
# ÄÃ¢y lÃ  chá»— ta dáº¡y model nÃ³i chuyá»‡n theo phong cÃ¡ch báº¡n muá»‘n



def chat_loop():
    print("\n>>> ðŸ¤– System Ready! GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.")

    # Táº¡o Query Engine (Bá»™ mÃ¡y truy váº¥n)
    # similarity_top_k=3 nghÄ©a lÃ  chá»‰ láº¥y 3 Ä‘oáº¡n vÄƒn báº£n giá»‘ng nháº¥t Ä‘á»ƒ gá»­i cho AI
    query_engine = index.as_query_engine(
        text_qa_template=qa_template,
        similarity_top_k=3
    )

    while True:
        user_input = input("\nBáº¡n: ")
        if user_input.lower() in ["exit", "quit"]:
            break

        # MAGIC HAPPENS HERE:
        response = query_engine.query(user_input)

        print(f"\nSyezain AI: {response}")


if __name__ == "__main__":
    chat_loop()