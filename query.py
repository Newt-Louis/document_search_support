# query.py
import logging
import sys
from llama_index.core import VectorStoreIndex, Settings
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama
import qdrant_client
from llama_index.core.prompts import PromptTemplate

# 1. Cáº¥u hÃ¬nh y há»‡t nhÆ° lÃºc Ingest (Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»“ng bá»™ vector)
Settings.embed_model = OllamaEmbedding(
    model_name="bge-m3",
    base_url="http://localhost:11434"
)

# Cáº¥u hÃ¬nh LLM: Llama-3.2 cháº¡y local
Settings.llm = Ollama(
    model="llama3.2",
    request_timeout=360.0,
    temperature=0.1  # Giá»¯ nhiá»‡t Ä‘á»™ tháº¥p Ä‘á»ƒ model tráº£ lá»i trung thá»±c, Ã­t sÃ¡ng táº¡o linh tinh
)

# 2. Káº¿t ná»‘i láº¡i vÃ o Qdrant (Chá»‰ connect, khÃ´ng táº¡o má»›i)
client = qdrant_client.QdrantClient(url="http://localhost:6333")
vector_store = QdrantVectorStore(client=client, collection_name="company_docs")

# 3. Load Index tá»« Vector DB lÃªn (SiÃªu nháº¹, khÃ´ng tá»‘n RAM load data gá»‘c)
index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

# 4. Cáº¥u hÃ¬nh Prompt "Tháº§n thÃ¡nh" (System Prompt)
# ÄÃ¢y lÃ  chá»— ta dáº¡y model nÃ³i chuyá»‡n theo phong cÃ¡ch báº¡n muá»‘n
qa_prompt_str = (
    "Báº¡n lÃ  trá»£ lÃ½ AI ná»™i bá»™. DÆ°á»›i Ä‘Ã¢y lÃ  thÃ´ng tin ngá»¯ cáº£nh (context) láº¥y tá»« tÃ i liá»‡u cÃ´ng ty:\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Dá»±a trÃªn ngá»¯ cáº£nh trÃªn (vÃ  CHá»ˆ dá»±a trÃªn ngá»¯ cáº£nh Ä‘Ã³), hÃ£y tráº£ lá»i cÃ¢u há»i: {query_str}\n\n"
    "YÃªu cáº§u báº¯t buá»™c:\n"
    "- Báº¯t Ä‘áº§u cÃ¢u tráº£ lá»i báº±ng cá»¥m tá»«: 'Theo nhÆ° thÃ´ng tin tÃ´i tÃ¬m Ä‘Æ°á»£c tá»« tÃ i liá»‡u ná»™i bá»™...'\n"
    "- Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong ngá»¯ cáº£nh, hÃ£y nÃ³i: 'Xin lá»—i, tÃ i liá»‡u hiá»‡n táº¡i khÃ´ng chá»©a thÃ´ng tin nÃ y.'\n"
    "- Káº¿t thÃºc báº±ng: 'ÄÃ¢y lÃ  táº¥t cáº£ thÃ´ng tin tÃ´i cÃ³ Ä‘Æ°á»£c, mong sáº½ giÃºp Ã­ch cho báº¡n.'"
)
qa_template = PromptTemplate(qa_prompt_str)


def chat_loop():
    print("\n>>> ðŸ¤– System Ready! GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.")

    # Táº¡o Query Engine (Bá»™ mÃ¡y truy váº¥n)
    # similarity_top_k=3 nghÄ©a lÃ  chá»‰ láº¥y 3 Ä‘oáº¡n vÄƒn báº£n giá»‘ng nháº¥t Ä‘á»ƒ gá»­i cho AI
    query_engine = index.as_query_engine(
        text_qa_template=qa_template,
        similarity_top_k=3
    )

    while True:
        user_input = input("\nBáº¡n: ")
        if user_input.lower() in ["exit", "quit"]:
            break

        # MAGIC HAPPENS HERE:
        response = query_engine.query(user_input)

        print(f"\nSyezain AI: {response}")


if __name__ == "__main__":
    chat_loop()